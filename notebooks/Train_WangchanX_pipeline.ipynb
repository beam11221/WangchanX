{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05rO24cDCZvq",
        "outputId": "60ab3ab0-416e-4e68-963c-9ca71b327d2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'WangchanX'...\n",
            "remote: Enumerating objects: 238, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 238 (delta 4), reused 4 (delta 2), pack-reused 226\u001b[K\n",
            "Receiving objects: 100% (238/238), 81.04 KiB | 691.00 KiB/s, done.\n",
            "Resolving deltas: 100% (109/109), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/vistec-AI/WangchanX.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd WangchanX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouEtP6h6CvMc",
        "outputId": "b7bf843b-c352-4c81-9e5d-10e703000745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/WangchanX\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Z3wnhN0oCsJj",
        "outputId": "e7cefa2b-6f1f-48e2-bcf7-24dfb2f7e1a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate>=0.29.2 (from -r requirements.txt (line 1))\n",
            "  Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes==0.41.2.post2 (from -r requirements.txt (line 2))\n",
            "  Downloading bitsandbytes-0.41.2.post2-py3-none-any.whl (92.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting black==23.1.0 (from -r requirements.txt (line 3))\n",
            "  Downloading black-23.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=2.18.0 (from -r requirements.txt (line 4))\n",
            "  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deepspeed==0.12.2 (from -r requirements.txt (line 5))\n",
            "  Downloading deepspeed-0.12.2.tar.gz (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting einops>=0.6.1 (from -r requirements.txt (line 6))\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate==0.4.0 (from -r requirements.txt (line 7))\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hf-doc-builder>=0.4.0 (from -r requirements.txt (line 8))\n",
            "  Downloading hf_doc_builder-0.5.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hf_transfer>=0.1.4 (from -r requirements.txt (line 9))\n",
            "  Downloading hf_transfer-0.1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.19.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (0.20.3)\n",
            "Collecting isort>=5.12.0 (from -r requirements.txt (line 11))\n",
            "  Downloading isort-5.13.2-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.3/92.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja>=1.11.1 (from -r requirements.txt (line 12))\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (1.25.2)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (24.0)\n",
            "Collecting parameterized>=0.9.0 (from -r requirements.txt (line 15))\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Collecting peft==0.7.1 (from -r requirements.txt (line 16))\n",
            "  Downloading peft-0.7.1-py3-none-any.whl (168 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf<=3.20.2 (from -r requirements.txt (line 17))\n",
            "  Downloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (0.4.3)\n",
            "Requirement already satisfied: sentencepiece>=0.1.99 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (0.1.99)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 20)) (1.11.4)\n",
            "Requirement already satisfied: transformers>=4.39.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (4.40.0)\n",
            "Collecting trl>=0.8.2 (from -r requirements.txt (line 22))\n",
            "  Downloading trl-0.8.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 23)) (3.1.3)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 24)) (4.66.2)\n",
            "Collecting wandb (from -r requirements.txt (line 25))\n",
            "  Downloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black==23.1.0->-r requirements.txt (line 3)) (8.1.7)\n",
            "Collecting mypy-extensions>=0.4.3 (from black==23.1.0->-r requirements.txt (line 3))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting pathspec>=0.9.0 (from black==23.1.0->-r requirements.txt (line 3))\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black==23.1.0->-r requirements.txt (line 3)) (4.2.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black==23.1.0->-r requirements.txt (line 3)) (2.0.1)\n",
            "Collecting hjson (from deepspeed==0.12.2->-r requirements.txt (line 5))\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.12.2->-r requirements.txt (line 5)) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.12.2->-r requirements.txt (line 5)) (9.0.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.12.2->-r requirements.txt (line 5)) (2.7.0)\n",
            "Collecting pynvml (from deepspeed==0.12.2->-r requirements.txt (line 5))\n",
            "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.12.2->-r requirements.txt (line 5)) (2.2.1+cu121)\n",
            "Collecting dill (from evaluate==0.4.0->-r requirements.txt (line 7))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.0->-r requirements.txt (line 7)) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.0->-r requirements.txt (line 7)) (2.31.0)\n",
            "Collecting xxhash (from evaluate==0.4.0->-r requirements.txt (line 7))\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from evaluate==0.4.0->-r requirements.txt (line 7))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.0->-r requirements.txt (line 7)) (2023.6.0)\n",
            "Collecting responses<0.19 (from evaluate==0.4.0->-r requirements.txt (line 7))\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.7.1->-r requirements.txt (line 16)) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.18.0->-r requirements.txt (line 4)) (3.13.4)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.18.0->-r requirements.txt (line 4)) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.18.0->-r requirements.txt (line 4)) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.18.0->-r requirements.txt (line 4)) (3.9.5)\n",
            "Collecting huggingface-hub<1.0,>=0.19.2 (from -r requirements.txt (line 10))\n",
            "  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting GitPython (from hf-doc-builder>=0.4.0->-r requirements.txt (line 8))\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from hf-doc-builder>=0.4.0->-r requirements.txt (line 8)) (5.10.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.2->-r requirements.txt (line 10)) (4.11.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.3->-r requirements.txt (line 21)) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.3->-r requirements.txt (line 21)) (0.19.1)\n",
            "Collecting tyro>=0.5.11 (from trl>=0.8.2->-r requirements.txt (line 22))\n",
            "  Downloading tyro-0.8.3-py3-none-any.whl (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.0/102.0 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=3.0.0->-r requirements.txt (line 23)) (2.1.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb->-r requirements.txt (line 25))\n",
            "  Downloading sentry_sdk-2.0.1-py2.py3-none-any.whl (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.8/266.8 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->-r requirements.txt (line 25))\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle (from wandb->-r requirements.txt (line 25))\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 25)) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 25)) (1.4.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 25)) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.18.0->-r requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.18.0->-r requirements.txt (line 4)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.18.0->-r requirements.txt (line 4)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.18.0->-r requirements.txt (line 4)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.18.0->-r requirements.txt (line 4)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.18.0->-r requirements.txt (line 4)) (4.0.3)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython->hf-doc-builder>=0.4.0->-r requirements.txt (line 8))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate==0.4.0->-r requirements.txt (line 7)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate==0.4.0->-r requirements.txt (line 7)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate==0.4.0->-r requirements.txt (line 7)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate==0.4.0->-r requirements.txt (line 7)) (2024.2.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.12.2->-r requirements.txt (line 5)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.12.2->-r requirements.txt (line 5)) (3.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->deepspeed==0.12.2->-r requirements.txt (line 5))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->deepspeed==0.12.2->-r requirements.txt (line 5))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->deepspeed==0.12.2->-r requirements.txt (line 5))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->deepspeed==0.12.2->-r requirements.txt (line 5))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->deepspeed==0.12.2->-r requirements.txt (line 5))\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->deepspeed==0.12.2->-r requirements.txt (line 5))\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->deepspeed==0.12.2->-r requirements.txt (line 5))\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->deepspeed==0.12.2->-r requirements.txt (line 5))\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->deepspeed==0.12.2->-r requirements.txt (line 5))\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->deepspeed==0.12.2->-r requirements.txt (line 5))\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->deepspeed==0.12.2->-r requirements.txt (line 5))\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.12.2->-r requirements.txt (line 5)) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->deepspeed==0.12.2->-r requirements.txt (line 5))\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl>=0.8.2->-r requirements.txt (line 22)) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl>=0.8.2->-r requirements.txt (line 22)) (13.7.1)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.8.2->-r requirements.txt (line 22))\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->hf-doc-builder>=0.4.0->-r requirements.txt (line 8)) (2.19.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->hf-doc-builder>=0.4.0->-r requirements.txt (line 8)) (4.19.2)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from nbformat->hf-doc-builder>=0.4.0->-r requirements.txt (line 8)) (5.7.2)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbformat->hf-doc-builder>=0.4.0->-r requirements.txt (line 8)) (5.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate==0.4.0->-r requirements.txt (line 7)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate==0.4.0->-r requirements.txt (line 7)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate==0.4.0->-r requirements.txt (line 7)) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed==0.12.2->-r requirements.txt (line 5)) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed==0.12.2->-r requirements.txt (line 5)) (2.18.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython->hf-doc-builder>=0.4.0->-r requirements.txt (line 8))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->hf-doc-builder>=0.4.0->-r requirements.txt (line 8)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->hf-doc-builder>=0.4.0->-r requirements.txt (line 8)) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->hf-doc-builder>=0.4.0->-r requirements.txt (line 8)) (0.18.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl>=0.8.2->-r requirements.txt (line 22)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl>=0.8.2->-r requirements.txt (line 22)) (2.16.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->deepspeed==0.12.2->-r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl>=0.8.2->-r requirements.txt (line 22)) (0.1.2)\n",
            "Building wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.12.2-py3-none-any.whl size=1265675 sha256=917992f04a0b206b18dcfa398bf3162aaf182c1fa29edcb8791a11298c0272ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/c8/39/10f68166de0a2a12c511c3c0569128b62be534edc9a224782c\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: ninja, hjson, bitsandbytes, xxhash, smmap, shtab, setproctitle, sentry-sdk, pynvml, protobuf, pathspec, parameterized, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, isort, hf_transfer, einops, docker-pycreds, dill, responses, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, huggingface-hub, gitdb, black, tyro, nvidia-cusolver-cu12, GitPython, wandb, datasets, hf-doc-builder, evaluate, deepspeed, accelerate, trl, peft\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.43 accelerate-0.29.3 bitsandbytes-0.41.2.post2 black-23.1.0 datasets-2.19.0 deepspeed-0.12.2 dill-0.3.8 docker-pycreds-0.4.0 einops-0.7.0 evaluate-0.4.0 gitdb-4.0.11 hf-doc-builder-0.5.0 hf_transfer-0.1.6 hjson-3.1.0 huggingface-hub-0.22.2 isort-5.13.2 multiprocess-0.70.16 mypy-extensions-1.0.0 ninja-1.11.1.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 parameterized-0.9.0 pathspec-0.12.1 peft-0.7.1 protobuf-3.20.2 pynvml-11.5.0 responses-0.18.0 sentry-sdk-2.0.1 setproctitle-1.3.3 shtab-1.7.1 smmap-5.0.1 trl-0.8.6 tyro-0.8.3 wandb-0.16.6 xxhash-3.4.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "d841ae7db9304004809a2626535bcc10"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6ExkjXfGHmK",
        "outputId": "4aaef758-43ee-4960-c45e-b8e2393e162b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.5.8.tar.gz (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.2.1+cu121)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn) (0.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from flash-attn) (24.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from flash-attn) (1.11.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.5.8-cp310-cp310-linux_x86_64.whl size=120853537 sha256=53979129f883680327bf5d13027cd014e2d054f4fb5b8856916686ae315e57d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/5b/2b/dea8af4e954161c49ef1941938afcd91bb93689371ed12a226\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: flash-attn\n",
            "Successfully installed flash-attn-2.5.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ในเตรียมดาต้า เรา Demo เทรนด้วยข้อมูลชุดนี้ https://huggingface.co/datasets/airesearch/concat_six_dataset_th_en ประกอบด้วยข้อมูล 6 ชุด รายละเอียดสามารถเข้าไปดูได้ครับ แต่เนื่องจาก หากท่านใดต้องการเทรนบน Colab หากชุดข้อมูลเยอะไปจะทำให้ใช้เวลานานในการเทรน ในการทำ Demo ใน Colab นี้เราจะใช้ชุดข้อมูลจาก ห่านดาต้าเซต https://huggingface.co/datasets/pythainlp/han-instruct-dataset-v2.0  ซึ่งเราได้ทำการ Re-format ไว้เรียบร้อยแล้วใน Folder dataset ในกรณีที่จะใช้ custom dataset ของตัวเอง สามารถแก้ไข code ใน reformate.py แล้วรันได้"
      ],
      "metadata": {
        "id": "KcoO-MjQCzyQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "หากท่านต้องการใช้ model Llama3 ในการเป็น base model ท่านต้องเข้าไปกดขอ permission ที่ https://huggingface.co/meta-llama/Meta-Llama-3-8B"
      ],
      "metadata": {
        "id": "w_8vH251GBXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token \"YOUR_TOKEN\" #หากต้องการใช้โมเดล Llama3 รันยืนยันตัวเองที่เซลนี้"
      ],
      "metadata": {
        "id": "yUsbCpmWFO8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ในการทำ Demo นี้เราจะใช้ model จาก scb10x/typhoon-7b ด้วย Qlora\n",
        "\n",
        "ท่านต้องเข้าไปแก้ Config ไฟล์ recipes/typhoon-7b/sft/config_qlora.yaml ใส่ Path dataset ให้ถูกต้อง เช่น ในบรรทัดที่ 26 จะเป็น dataset/han_train.json : 1.0 และในไฟล์เดียวกันท่านสามารถแก้ไข per_device_train_batch_size: 1 ได้ หากท่านมี GPU ที่ไม่เพียงพอ"
      ],
      "metadata": {
        "id": "l0KVf2T2G9r9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd WangchanX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnnetR_LIveb",
        "outputId": "84ca5bf5-072e-47bc-afcc-59fbbf98b956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/WangchanX\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "หากโมเดลใช้เวลาเทรนนาน หรือ GPU memory ไม่พอ ท่านสามารถพิจารณาในการลด R : 8, alpha : 8 พารามิเตอร์ของ Qlora ได้ ซึ่งสามารถแก้ไขได้ที่ไฟล์ config .yaml ของโมเดลนั้นๆ ใน Demo เราจะแก้ training epoch เป็น 1"
      ],
      "metadata": {
        "id": "wcbLGol9MRIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/run_sft.py recipes/typhoon-7b/sft/config_qlora.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdaagyluHGh7",
        "outputId": "2f94c687-def5-4909-c3cb-49888ed77733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-27 15:50:18.038430: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-27 15:50:18.038486: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-27 15:50:18.040265: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-27 15:50:19.269152: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2024-04-27 15:50:19,855] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "2024-04-27 15:50:20 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False\n",
            "2024-04-27 15:50:20 - INFO - __main__ - Model parameters ModelArguments(base_model_revision=None, model_name_or_path='scb10x/typhoon-7b', model_revision='main', model_code_revision=None, torch_dtype='bfloat16', tokenizer_name_or_path=None, trust_remote_code=False, use_flash_attention_2=True, use_peft=True, lora_r=16, lora_alpha=16, lora_dropout=0.05, lora_target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'], lora_modules_to_save=None, load_in_8bit=False, load_in_4bit=True, bnb_4bit_quant_type='nf4', use_bnb_nested_quant=False)\n",
            "2024-04-27 15:50:20 - INFO - __main__ - Data parameters DataArguments(chat_template=\"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\", dataset_mixer={'dataset/han_train.json': 1.0}, text_column='text', dataset_splits=['train'], dataset_configs=None, preprocessing_num_workers=12, truncation_side=None, auto_insert_empty_system_msg=True)\n",
            "2024-04-27 15:50:20 - INFO - __main__ - Training/evaluation parameters SFTConfig(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=True,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "dataset_kwargs=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=32,\n",
            "gradient_checkpointing=True,\n",
            "gradient_checkpointing_kwargs={'use_reentrant': False},\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0002,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=info,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=data/typhoon-7b-sft-qlora/runs/Apr27_15-50-20_eef658a497e1,\n",
            "logging_first_step=True,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=50,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=cosine,\n",
            "max_grad_norm=1.0,\n",
            "max_seq_length=2048,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=data/typhoon-7b-sft-qlora,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=2,\n",
            "per_device_train_batch_size=2,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=data/typhoon-7b-sft-qlora,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=10,\n",
            "save_strategy=steps,\n",
            "save_total_limit=1,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.1,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "2024-04-27 15:50:20 - INFO - __main__ - Checkpoint detected, resuming training at data/typhoon-7b-sft-qlora/checkpoint-10.\n",
            "Using custom data configuration default-c96f17c197c22583\n",
            "2024-04-27 15:50:20 - INFO - datasets.builder - Using custom data configuration default-c96f17c197c22583\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "2024-04-27 15:50:20 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "2024-04-27 15:50:20 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7\n",
            "2024-04-27 15:50:20 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)\n",
            "2024-04-27 15:50:20 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7\n",
            "2024-04-27 15:50:20 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7\n",
            "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-25487182f6f93584.arrow\n",
            "2024-04-27 15:50:20 - INFO - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-25487182f6f93584.arrow\n",
            "2024-04-27 15:50:20 - INFO - __main__ - Training on the following datasets and their proportions: ['train : 3200']\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-04-27 15:50:20,754 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--scb10x--typhoon-7b/snapshots/f28580fc505e664cf13c11bcc0059ef839354837/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-04-27 15:50:20,754 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--scb10x--typhoon-7b/snapshots/f28580fc505e664cf13c11bcc0059ef839354837/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-04-27 15:50:20,754 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-04-27 15:50:20,754 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--scb10x--typhoon-7b/snapshots/f28580fc505e664cf13c11bcc0059ef839354837/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-04-27 15:50:20,754 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--scb10x--typhoon-7b/snapshots/f28580fc505e664cf13c11bcc0059ef839354837/tokenizer_config.json\n",
            "2024-04-27 15:50:20 - INFO - __main__ - *** Load pretrained model ***\n",
            "Process #0 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0b19f59b07c156d4_00000_of_00012.arrow\n",
            "2024-04-27 15:50:20 - INFO - datasets.arrow_dataset - Process #0 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0b19f59b07c156d4_00000_of_00012.arrow\n",
            "Process #1 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0b19f59b07c156d4_00001_of_00012.arrow\n",
            "2024-04-27 15:50:20 - INFO - datasets.arrow_dataset - Process #1 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0b19f59b07c156d4_00001_of_00012.arrow\n",
            "Process #2 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0b19f59b07c156d4_00002_of_00012.arrow\n",
            "2024-04-27 15:50:20 - INFO - datasets.arrow_dataset - Process #2 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0b19f59b07c156d4_00002_of_00012.arrow\n",
            "Process #3 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0b19f59b07c156d4_00003_of_00012.arrow\n",
            "2024-04-27 15:50:20 - INFO - datasets.arrow_dataset - Process #3 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0b19f59b07c156d4_00003_of_00012.arrow\n",
            "Process #4 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0b19f59b07c156d4_00004_of_00012.arrow\n",
            "2024-04-27 15:50:20 - INFO - datasets.arrow_dataset - Process #4 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0b19f59b07c156d4_00004_of_00012.arrow\n",
            "Process #5 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0b19f59b07c156d4_00005_of_00012.arrow\n",
            "2024-04-27 15:50:20 - INFO - datasets.arrow_dataset - Process #5 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0b19f59b07c156d4_00005_of_00012.arrow\n",
            "Process #6 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0b19f59b07c156d4_00006_of_00012.arrow\n",
            "2024-04-27 15:50:20 - INFO - datasets.arrow_dataset - Process #6 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0b19f59b07c156d4_00006_of_00012.arrow\n",
            "Process #7 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0b19f59b07c156d4_00007_of_00012.arrow\n",
            "2024-04-27 15:50:20 - INFO - datasets.arrow_dataset - Process #7 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0b19f59b07c156d4_00007_of_00012.arrow\n",
            "Process #8 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0b19f59b07c156d4_00008_of_00012.arrow\n",
            "2024-04-27 15:50:20 - INFO - datasets.arrow_dataset - Process #8 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0b19f59b07c156d4_00008_of_00012.arrow\n",
            "Process #9 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0b19f59b07c156d4_00009_of_00012.arrow\n",
            "2024-04-27 15:50:20 - INFO - datasets.arrow_dataset - Process #9 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0b19f59b07c156d4_00009_of_00012.arrow\n",
            "Process #10 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0b19f59b07c156d4_00010_of_00012.arrow\n",
            "2024-04-27 15:50:20 - INFO - datasets.arrow_dataset - Process #10 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0b19f59b07c156d4_00010_of_00012.arrow\n",
            "Process #11 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0b19f59b07c156d4_00011_of_00012.arrow\n",
            "2024-04-27 15:50:20 - INFO - datasets.arrow_dataset - Process #11 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0b19f59b07c156d4_00011_of_00012.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0b19f59b07c156d4_*_of_00012.arrow\n",
            "2024-04-27 15:50:20 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0b19f59b07c156d4_*_of_00012.arrow\n",
            "Concatenating 12 shards\n",
            "2024-04-27 15:50:20 - INFO - datasets.arrow_dataset - Concatenating 12 shards\n",
            "Process #0 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00000_of_00008.arrow\n",
            "2024-04-27 15:50:20 - INFO - datasets.arrow_dataset - Process #0 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00000_of_00008.arrow\n",
            "Process #1 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00001_of_00008.arrow\n",
            "2024-04-27 15:50:20 - INFO - datasets.arrow_dataset - Process #1 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00001_of_00008.arrow\n",
            "Process #2 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00002_of_00008.arrow\n",
            "2024-04-27 15:50:20 - INFO - datasets.arrow_dataset - Process #2 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00002_of_00008.arrow\n",
            "Process #3 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00003_of_00008.arrow\n",
            "2024-04-27 15:50:20 - INFO - datasets.arrow_dataset - Process #3 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00003_of_00008.arrow\n",
            "Process #4 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00004_of_00008.arrow\n",
            "2024-04-27 15:50:20 - INFO - datasets.arrow_dataset - Process #4 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00004_of_00008.arrow\n",
            "Process #5 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00005_of_00008.arrow\n",
            "2024-04-27 15:50:20 - INFO - datasets.arrow_dataset - Process #5 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00005_of_00008.arrow\n",
            "Process #6 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00006_of_00008.arrow\n",
            "2024-04-27 15:50:20 - INFO - datasets.arrow_dataset - Process #6 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00006_of_00008.arrow\n",
            "Process #7 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00007_of_00008.arrow\n",
            "2024-04-27 15:50:20 - INFO - datasets.arrow_dataset - Process #7 will write at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00007_of_00008.arrow\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Spawning 8 processes\n",
            "2024-04-27 15:50:21 - INFO - datasets.arrow_dataset - Spawning 8 processes\n",
            "Filter (num_proc=8):   0% 0/3200 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00000_of_00008.arrow\n",
            "2024-04-27 15:50:21 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00000_of_00008.arrow\n",
            "Filter (num_proc=8):  12% 400/3200 [00:00<00:02, 1249.40 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00001_of_00008.arrow\n",
            "2024-04-27 15:50:21 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00001_of_00008.arrow\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00002_of_00008.arrow\n",
            "2024-04-27 15:50:21 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00002_of_00008.arrow\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00003_of_00008.arrow\n",
            "2024-04-27 15:50:21 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00003_of_00008.arrow\n",
            "Filter (num_proc=8):  50% 1600/3200 [00:00<00:00, 4326.86 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00004_of_00008.arrow\n",
            "2024-04-27 15:50:21 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00004_of_00008.arrow\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00005_of_00008.arrow\n",
            "2024-04-27 15:50:21 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00005_of_00008.arrow\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00006_of_00008.arrow\n",
            "2024-04-27 15:50:21 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00006_of_00008.arrow\n",
            "Filter (num_proc=8):  88% 2800/3200 [00:00<00:00, 6481.86 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00007_of_00008.arrow\n",
            "2024-04-27 15:50:21 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-38c9ad8739402c4c_00007_of_00008.arrow\n",
            "Filter (num_proc=8): 100% 3200/3200 [00:00<00:00, 4984.42 examples/s]\n",
            "Concatenating 8 shards\n",
            "2024-04-27 15:50:21 - INFO - datasets.arrow_dataset - Concatenating 8 shards\n",
            "2024-04-27 15:50:21 - INFO - __main__ - Sample 2619 of the processed training set:\n",
            "\n",
            "<|system|>\n",
            "</s>\n",
            "<|user|>\n",
            "พันเจียคืออะไร</s>\n",
            "<|assistant|>\n",
            "พันเจียคือทวีปของโลกที่แผ่นดินเป็นแผ่นเดียวกันเมื่อ 200-135 ล้านปีที่แล้ว ตามทฤษฎีทวีปเลื่อน</s>\n",
            "\n",
            "2024-04-27 15:50:21 - INFO - __main__ - Sample 456 of the processed training set:\n",
            "\n",
            "<|system|>\n",
            "</s>\n",
            "<|user|>\n",
            "เขียนบทความเกี่ยวกับ \"ประโยชน์ของโกจิเบอร์รี่\"</s>\n",
            "<|assistant|>\n",
            "ประโยชน์จากโก จิเบอร์รี โก-จิ เบอร์ รี (Goji berry) เป็นผลไม้ที่มีถิ่นกําเนิดในประเทศจีน นิยมรับประทานกันมากในแถบเอเชียตะวันออกเฉียงใต้ ในประเทศไทย ชาวจีนนิยมนําผลโก จี เบิร์ด มารับประทานเป็นผลไม้สด หรือนํามาแปรรูปเป็นเครื่องดื่มต่างๆ เช่น น้ําผลไม้ เครื่องดื่มชูกําลัง นมเปรี้ยว โยเกิร์ต เป็นต้น สรรพคุณของ ผลไม้ชนิดนี้ มีมากมายหลายประการ เช่น ช่วยบํารุงสายตา บํารุงผิวพรรณ ป้องกันโรคมะเร็ง ต้านอนุมูลอิสระ เสริมสร้างภูมิคุ้มกัน กระตุ้นระบบภูมิต้านทาน ช่วยให้นอนหลับสบาย แก้อาการอ่อนเพลีย บรรเทาอาการปวดข้อ ปวดกระดูก ขับปัสสาวะ ลดระดับน้ําตาลในเลือด ควบคุมน้ําหนัก ปรับสมดุลฮอร์โมน ส่งเสริมการทํางานของระบบประสาทและสมอง ที่สําคัญคือมีสารต้านมะเร็งสูงมาก</s>\n",
            "\n",
            "2024-04-27 15:50:21 - INFO - __main__ - Sample 102 of the processed training set:\n",
            "\n",
            "<|system|>\n",
            "</s>\n",
            "<|user|>\n",
            "จงตอบคำถามต่อไปนี้เป็นภาษาอังกฤษเท่านั้น:\n",
            "โลกมีอายุประมาณกี่ปี</s>\n",
            "<|assistant|>\n",
            "Earth is estimated to be 4.54 billion years old.</s>\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:166: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:726] 2024-04-27 15:50:21,770 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--scb10x--typhoon-7b/snapshots/f28580fc505e664cf13c11bcc0059ef839354837/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-04-27 15:50:21,771 >> Model config MistralConfig {\n",
            "  \"_name_or_path\": \"scb10x/typhoon-7b\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.40.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 35219\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3429] 2024-04-27 15:50:21,776 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--scb10x--typhoon-7b/snapshots/f28580fc505e664cf13c11bcc0059ef839354837/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1494] 2024-04-27 15:50:21,777 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
            "[WARNING|logging.py:329] 2024-04-27 15:50:21,777 >> The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
            "[INFO|configuration_utils.py:928] 2024-04-27 15:50:21,781 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:07<00:00,  3.52s/it]\n",
            "[INFO|modeling_utils.py:4170] 2024-04-27 15:50:29,642 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4178] 2024-04-27 15:50:29,642 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at scb10x/typhoon-7b.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:883] 2024-04-27 15:50:29,735 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--scb10x--typhoon-7b/snapshots/f28580fc505e664cf13c11bcc0059ef839354837/generation_config.json\n",
            "[INFO|configuration_utils.py:928] 2024-04-27 15:50:29,735 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2\n",
            "}\n",
            "\n",
            "[WARNING|quantizer_bnb_4bit.py:307] 2024-04-27 15:50:29,980 >> You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n",
            "[INFO|training_args.py:1997] 2024-04-27 15:50:29,995 >> PyTorch: setting up devices\n",
            "Map:   0% 0/3200 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aac3e4d0f92c6c1a.arrow\n",
            "2024-04-27 15:50:30 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-c96f17c197c22583/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aac3e4d0f92c6c1a.arrow\n",
            "Map: 100% 3200/3200 [00:00<00:00, 3739.37 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:626] 2024-04-27 15:50:31,845 >> Using auto half precision backend\n",
            "2024-04-27 15:50:31 - INFO - __main__ - *** Train ***\n",
            "[INFO|trainer.py:2433] 2024-04-27 15:50:31,846 >> Loading model from data/typhoon-7b-sft-qlora/checkpoint-10.\n",
            "[INFO|trainer.py:2048] 2024-04-27 15:50:32,482 >> ***** Running training *****\n",
            "[INFO|trainer.py:2049] 2024-04-27 15:50:32,482 >>   Num examples = 3,200\n",
            "[INFO|trainer.py:2050] 2024-04-27 15:50:32,482 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:2051] 2024-04-27 15:50:32,482 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2054] 2024-04-27 15:50:32,482 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "[INFO|trainer.py:2055] 2024-04-27 15:50:32,482 >>   Gradient Accumulation steps = 32\n",
            "[INFO|trainer.py:2056] 2024-04-27 15:50:32,482 >>   Total optimization steps = 50\n",
            "[INFO|trainer.py:2057] 2024-04-27 15:50:32,489 >>   Number of trainable parameters = 41,943,040\n",
            "[INFO|trainer.py:2078] 2024-04-27 15:50:32,489 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:2079] 2024-04-27 15:50:32,489 >>   Continuing training from epoch 0\n",
            "[INFO|trainer.py:2080] 2024-04-27 15:50:32,489 >>   Continuing training from global step 10\n",
            "[INFO|trainer.py:2082] 2024-04-27 15:50:32,489 >>   Will skip the first 0 epochs then the first 320 batches in the first epoch.\n",
            "[INFO|integration_utils.py:723] 2024-04-27 15:50:32,495 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "  0% 0/50 [00:00<?, ?it/s][WARNING|logging.py:329] 2024-04-27 15:50:40,565 >> The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
            " 40% 20/50 [08:48<23:37, 47.26s/it][INFO|trainer.py:3305] 2024-04-27 15:59:28,002 >> Saving model checkpoint to data/typhoon-7b-sft-qlora/checkpoint-20\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-27 15:59:28,231 >> tokenizer config file saved in data/typhoon-7b-sft-qlora/checkpoint-20/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-27 15:59:28,232 >> Special tokens file saved in data/typhoon-7b-sft-qlora/checkpoint-20/special_tokens_map.json\n",
            "[INFO|trainer.py:3397] 2024-04-27 15:59:28,586 >> Deleting older checkpoint [data/typhoon-7b-sft-qlora/checkpoint-10] due to args.save_total_limit\n",
            " 60% 30/50 [17:48<18:21, 55.09s/it][INFO|trainer.py:3305] 2024-04-27 16:08:28,837 >> Saving model checkpoint to data/typhoon-7b-sft-qlora/checkpoint-30\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-27 16:08:29,018 >> tokenizer config file saved in data/typhoon-7b-sft-qlora/checkpoint-30/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-27 16:08:29,018 >> Special tokens file saved in data/typhoon-7b-sft-qlora/checkpoint-30/special_tokens_map.json\n",
            "[INFO|trainer.py:3397] 2024-04-27 16:08:29,376 >> Deleting older checkpoint [data/typhoon-7b-sft-qlora/checkpoint-20] due to args.save_total_limit\n",
            " 80% 40/50 [26:38<08:57, 53.78s/it][INFO|trainer.py:3305] 2024-04-27 16:17:18,669 >> Saving model checkpoint to data/typhoon-7b-sft-qlora/checkpoint-40\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-27 16:17:18,847 >> tokenizer config file saved in data/typhoon-7b-sft-qlora/checkpoint-40/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-27 16:17:18,847 >> Special tokens file saved in data/typhoon-7b-sft-qlora/checkpoint-40/special_tokens_map.json\n",
            "[INFO|trainer.py:3397] 2024-04-27 16:17:19,202 >> Deleting older checkpoint [data/typhoon-7b-sft-qlora/checkpoint-30] due to args.save_total_limit\n",
            "{'loss': 1.4883, 'grad_norm': 0.55078125, 'learning_rate': 0.0, 'epoch': 1.0}\n",
            "100% 50/50 [35:36<00:00, 52.92s/it][INFO|trainer.py:3305] 2024-04-27 16:26:16,580 >> Saving model checkpoint to data/typhoon-7b-sft-qlora/checkpoint-50\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-27 16:26:16,760 >> tokenizer config file saved in data/typhoon-7b-sft-qlora/checkpoint-50/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-27 16:26:16,760 >> Special tokens file saved in data/typhoon-7b-sft-qlora/checkpoint-50/special_tokens_map.json\n",
            "[INFO|trainer.py:3397] 2024-04-27 16:26:17,113 >> Deleting older checkpoint [data/typhoon-7b-sft-qlora/checkpoint-40] due to args.save_total_limit\n",
            "[INFO|trainer.py:2316] 2024-04-27 16:26:17,152 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2144.6637, 'train_samples_per_second': 1.492, 'train_steps_per_second': 0.023, 'train_loss': 1.1906317901611327, 'epoch': 1.0}\n",
            "100% 50/50 [35:37<00:00, 42.74s/it]\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  total_flos               = 19622803GF\n",
            "  train_loss               =     1.1906\n",
            "  train_runtime            = 0:35:44.66\n",
            "  train_samples            =       3200\n",
            "  train_samples_per_second =      1.492\n",
            "  train_steps_per_second   =      0.023\n",
            "2024-04-27 16:26:17 - INFO - __main__ - *** Save model ***\n",
            "[INFO|trainer.py:3305] 2024-04-27 16:26:17,159 >> Saving model checkpoint to data/typhoon-7b-sft-qlora\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-27 16:26:17,337 >> tokenizer config file saved in data/typhoon-7b-sft-qlora/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-27 16:26:17,338 >> Special tokens file saved in data/typhoon-7b-sft-qlora/special_tokens_map.json\n",
            "2024-04-27 16:26:17 - INFO - __main__ - Model saved to data/typhoon-7b-sft-qlora\n",
            "[INFO|modelcard.py:450] 2024-04-27 16:26:17,537 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'dataset': {'name': 'dataset/han_train.json', 'type': 'dataset/han_train.json'}}\n",
            "[INFO|configuration_utils.py:471] 2024-04-27 16:26:17,542 >> Configuration saved in data/typhoon-7b-sft-qlora/config.json\n",
            "2024-04-27 16:26:17 - INFO - __main__ - *** Training complete ***\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/epoch ▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/global_step ▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train/grad_norm ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/learning_rate ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               total_flos 2.106982489522176e+16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/epoch 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train/global_step 50\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/grad_norm 0.55078\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/loss 1.4883\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train_loss 1.19063\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train_runtime 2144.6637\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_samples_per_second 1.492\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train_steps_per_second 0.023\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[1mwandb sync /content/WangchanX/wandb/offline-run-20240427_155039-ix213wls\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/offline-run-20240427_155039-ix213wls/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference"
      ],
      "metadata": {
        "id": "bH84DDeTK56S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    \"data/typhoon-7b-sft-qlora\" ,\n",
        "     torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        "    )\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"scb10x/typhoon-7b\")\n",
        "tokenizer.chat_template = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
        "\n",
        "\n",
        "prompt =\"บอกวิธีการเลือกซื้อโทรศัพท์ใหม่\"\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
        "generated_ids = model.generate(\n",
        "    input_ids=encodeds,\n",
        "    max_new_tokens=300,\n",
        "    early_stopping=True,\n",
        "    top_k=50, top_p=0.95,\n",
        "    do_sample=True,\n",
        "    temperature=0.4,\n",
        "    repetition_penalty = 1.2,\n",
        "    eos_token_id =tokenizer.eos_token_id\n",
        ")\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded[0])"
      ],
      "metadata": {
        "id": "U4c5MEVqK09k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}